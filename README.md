# Human-Motion-Video-Generation
This repo aims to record SOTA works on the human motion video generation.

## Pose-Guided & Full Body

* [2023-04] Follow Your Pose: Pose-Guided Text-to-Video Generation using Pose-Free Videos | [Paper](https://arxiv.org/pdf/2304.01186) | [Code](https://github.com/mayuelala/FollowYourPose) ![Stars](https://img.shields.io/github/stars/mayuelala/FollowYourPose) AAAI'24

* [2023-05] ControlVideo: Adding Conditional Control for One Shot Text-to-Video Editing | [Paper](https://arxiv.org/pdf/2305.17098.pdf) | [Code](https://github.com/thu-ml/controlvideo) ![Stars](https://img.shields.io/github/stars/thu-ml/controlvideo) arXiv'23
  
* [2023-07] DISCO: Disentangled Control for Realistic Human Dance Generation | [Paper](http://openaccess.thecvf.com/content/CVPR2024/papers/Wang_DisCo_Disentangled_Control_for_Realistic_Human_Dance_Generation_CVPR_2024_paper.pdf) | [Code](https://github.com/Wangt-CN/DisCo) ![Stars](https://img.shields.io/github/stars/Wangt-CN/DisCo) CVPR'24
  
* [2023-11] MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-aware Diffusion | [Paper](https://arxiv.org/abs/2311.12052) | [Code](https://github.com/Boese0601/MagicDance) ![Stars](https://img.shields.io/github/stars/Boese0601/MagicDance) ICML'24
 
* [2023-11] MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model | [Paper](https://arxiv.org/abs/2311.16498) | [Code](https://github.com/magic-research/magic-animate) ![Stars](https://img.shields.io/github/stars/magic-research/magic-animate) CVPR'24

* [2023-11] Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation | [Paper](https://arxiv.org/pdf/2311.17117.pdf) | [Code](https://github.com/MooreThreads/Moore-AnimateAnyone) ![Stars](https://img.shields.io/github/stars/MooreThreads/Moore-AnimateAnyone) CVPR'24

* [2023-11] HumanGaussian: Text-Driven 3D Human Generation with Gaussian Splatting | [Code](https://github.com/alvinliu0/HumanGaussian) ![Stars](https://img.shields.io/github/stars/alvinliu0/HumanGaussian) CVPR'24

* [2023-12] GaussianAvatar: Towards Realistic Human Avatar Modeling from a Single Video via Animatable 3D Gaussians | [Paper](https://arxiv.org/abs/2312.02134) [Code](https://github.com/aipixel/GaussianAvatar) ![Stars](https://img.shields.io/github/stars/aipixel/GaussianAvatar) CVPR'24

* [2024-05] VividPose: Advancing Stable Video Diffusion for Realistic Human Image Animation | [Paper](https://arxiv.org/html/2405.18156v1)

* [2024-07] MimicMotion: High-Quality Human Motion Video Generation with Confidence-aware Pose Guidance | [Paper](https://arxiv.org/abs/2406.19680) | [Code](https://github.com/Tencent/MimicMotion) ![Stars](https://img.shields.io/github/stars/Tencent/MimicMotion)
  
* [2024-09] RealisDance: Equip Controllable Character Animation with Realistic Hands | [Paper](https://arxiv.org/pdf/2409.06202) | [Code](https://github.com/damo-cv/RealisDance) ![Stars](https://img.shields.io/github/stars/damo-cv/RealisDance)  

* [2024-MM] MusePose: a Pose-Driven Image-to-Video Framework for Virtual Human Generation [Code](https://github.com/TMElyralab/MusePose) ![Stars](https://img.shields.io/github/stars/TMElyralab/MusePose)
  
* [2025-02] HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation | [Paper](https://arxiv.org/pdf/2502.04847)

* [2025-02] AnyCharV: Bootstrap Controllable Character Video Generation with Fine-to-Coarse Guidance | [Paper](https://arxiv.org/pdf/2502.08189) | [Code](https://github.com/AnyCharV/AnyCharV) ![Stars](https://img.shields.io/github/stars/AnyCharV/AnyCharV)

* [2025-04] UniAnimate: Taming Unified Video Diffusion Models for Consistent Human Image Animation | [Code](https://github.com/ali-vilab/UniAnimate-DiT) ![Stars](https://img.shields.io/github/stars/ali-vilab/UniAnimate-DiT) Sci. China Inf. Sci.'25

* [2025-05] MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation | [Paper](https://arxiv.org/abs/2505.10238) | [Code](https://github.com/undefined) ![Stars](https://img.shields.io/github/stars/undefined) arXiv'25

## Talking Head
* [2023-11] MagicAnimate: Temporally Consistent Human Image Animation using Diffusion Model | [Paper](https://arxiv.org/abs/2311.16498) | [Code](https://github.com/magic-research/magic-animate) ![Stars](https://img.shields.io/github/stars/magic-research/magic-animate)

* [2024-02] EMO: Emote Portrait Alive - Generating Expressive Portrait Videos with Audio2Video Diffusion Model under Weak Conditions | [Paper](https://arxiv.org/pdf/2402.17485) | [Code](https://github.com/HumanAIGC/EMO)![Stars](https://img.shields.io/github/stars/HumanAIGC/EMO)

* [2024-11] EchoMimicV2: Towards Striking, Simplified, and Semi-Body Human Animation | [Paper](https://arxiv.org/abs/2411.10061) | [Code](https://github.com/antgroup/echomimic_v2) ![Stars](https://img.shields.io/github/stars/antgroup/echomimic_v2)  CVPR'25

* [2025-04] FantasyTalking: Realistic Talking Portrait Generation via Coherent Motion Synthesis | [Paper](https://arxiv.org/abs/2504.04842) | [Code](https://github.com/Fantasy-AMAP/fantasy-talking) ![Stars](https://img.shields.io/github/stars/Fantasy-AMAP/fantasy-talking?style=social)

## Virtual Try-On
* [2014] Garment Replacement in Monocular Video Sequences, | [Paper](https://dl.acm.org/doi/10.1145/2634212) TOG'14
  
* [2019] FW-GAN: Flow-Navigated Warping GAN for Video Virtual Try-On| [Paper](http://openaccess.thecvf.com/content_ICCV_2019/html/Dong_FW-GAN_Flow-Navigated_Warping_GAN_for_Video_Virtual_Try-On_ICCV_2019_paper.html) ICCV'19
  
* [2019] Unsupervised Image-to-Video Clothing Transfer | [Paper](http://openaccess.thecvf.com/content_ICCVW_2019/html/CVFAD/Pumarola_Unsupervised_Image-to-Video_Clothing_Transfer_ICCVW_2019_paper.html) ICCV'19
  
* [2021] ShineOn: Illuminating Design Choices for Practical Video-based Virtual Clothing Try-on | [Code](https://gauravkuppa.github.io/publication/2021-01-09-shine-on-1) WACVW'21
  
* [2021] MV-TON: Memory-based Video Virtual Try-on network, | [Paper](https://arxiv.org/abs/2108.07502) | MM'21
  
* [2022] ClothFormer: Taming Video Virtual Try-on in All Module | [Paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Jiang_ClothFormer_Taming_Video_Virtual_Try-On_in_All_Module_CVPR_2022_paper.pdf) | [Code](https://github.com/luxiangju-PersonAI/ClothFormer) | ![Stars](https://img.shields.io/github/stars/luxiangju-PersonAI/ClothFormer) CVPR'22

* [2024-05] ViViD: Video Virtual Try-on using Diffusion Models | [Paper](https://arxiv.org/abs/2405.11794) | [Code](https://github.com/alibaba-yuanjing-aigclab/ViViD)
  
* [2025-01] 1-2-1: Renaissance of Single-Network Paradigm for Virtual Try-On | [Paper](https://arxiv.org/pdf/2501.05369) | [Code](https://github.com/ningshuliang/1-2-1-MNVTON) ![Stars](https://img.shields.io/github/stars/ningshuliang/1-2-1-MNVTON)
  
* [2025-02] CatV2TON: Taming Diffusion Transformers for Vision-Based Virtual Try-On with Temporal Concatenation | [Paper](http://arxiv.org/abs/2501.11325v1) | [Code](https://github.com/zheng-chong/catv2ton) ![Stars](https://img.shields.io/github/stars/zheng-chong/catv2ton)



  ## Vision Language Model
* [2024-05] EasyAnimate: A High-Performance Long Video Generation Method based on Transformer Architecture | [Paper](https://arxiv.org/abs/2405.18991) | [Code](http://github.com/aigc-apps/EasyAnimate?tab=readme-ov-file) ![Stars](https://img.shields.io/github/stars/aigc-apps/EasyAnimate?tab=readme-ov-file)

* [2024-08] CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer | [Paper](https://arxiv.org/abs/2408.06072) | [Code](https://github.com/THUDM/CogVideo)![Stars](https://img.shields.io/github/stars/THUDM/CogVideo)
  
* [2024-12] HunyuanVideo: A Systematic Framework For Large Video Generation Model | [Paper](https://arxiv.org/abs/2412.03603) | [Code](https://github.com/Tencent/HunyuanVideo) ![Stars](https://img.shields.io/github/stars/Tencent/HunyuanVideo)

* [2024-12] Open-Sora Plan: Open-Source Large Video Generation Model | [Paper](https://arxiv.org/abs/2412.00131) | [Code](https://github.com/PKU-YuanGroup/Open-Sora-Plan)![Stars](https://img.shields.io/github/stars/PKU-YuanGroup/Open-Sora-Plan)

* [2025-02] Step-Video-T2V Technical Report: The Practice, Challenges, and Future of Video Foundation Model | [Paper](https://arxiv.org/abs/2502.10248) | [Code](https://github.com/stepfun-ai/Step-Video-T2V)![Stars](https://img.shields.io/github/stars/stepfun-ai/Step-Video-T2V)

  


